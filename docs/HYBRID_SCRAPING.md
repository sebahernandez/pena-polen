# SoluciÃ³n Final: Hybrid Scraping Strategy

## Problema Original
El endpoint `/api/scrape` en Vercel Serverless Functions estaba fallando con "This Serverless Function has crashed" debido a:

1. **Puppeteer es incompatible con Vercel Serverless**
   - Binary: ~300MB (excede lÃ­mites de Vercel)
   - Chrome necesita librerÃ­as del sistema que no existen en Vercel
   - Timeout: Las conexiones son demasiado lentas

2. **Intentos fallidos**
   - chrome-aws-lambda (150MB binary) â†’ Sigue causando crashes
   - puppeteer-core (version conflicts) â†’ Conflictos de dependencias
   - node-html-parser â†’ No interpreta JavaScript

## SoluciÃ³n Implementada

### Estrategia Hybrid: Dual-Mode Scraping

El archivo `src/lib/polenes.ts` ahora detecta el entorno y usa estrategias diferentes:

```typescript
export async function scrapePollenData(): Promise<PollenData | null> {
  const isVercel = process.env.VERCEL === '1';
  
  if (isVercel) {
    return await scrapeWithFetch();  // Lightweight, sin browser
  } else {
    return await scrapeWithPuppeteer();  // Full rendering, desarrollo local
  }
}
```

### Modo 1: `scrapeWithFetch()` (Vercel Production)
- âœ… **No requiere binarios**: Usa solo `fetch` nativo de Node.js
- âœ… **Ligero**: ~10KB de memoria
- âœ… **RÃ¡pido**: Timeout < 5 segundos
- âš ï¸ **LimitaciÃ³n**: No interpreta JavaScript; solo HTML estÃ¡tico

### Modo 2: `scrapeWithPuppeteer()` (Local Development)
- âœ… **Full JavaScript rendering**: Maneja contenido dinÃ¡mico
- âœ… **Confiable para desarrollo**: Funciona perfectamente
- âŒ **No funciona en Vercel**: Binarios demasiado grandes

## Cambios en Dependencias

### Removidas
```bash
npm uninstall chrome-aws-lambda puppeteer-core
```

### Resultado Final
- `puppeteer@24.23.0` - Solo para desarrollo local
- Sin dependencias de Vercel Lambda
- Sin conflictos de versiones

## ConfiguraciÃ³n Vercel Actualizada

`vercel.json` ahora usa memoria mÃ­nima:

```json
{
  "functions": {
    "src/pages/api/scrape.ts": {
      "maxDuration": 30,
      "memory": 512
    },
    "src/pages/api/penaflor.ts": {
      "maxDuration": 10,
      "memory": 256
    },
    "src/pages/api/history.ts": {
      "maxDuration": 10,
      "memory": 256
    }
  }
}
```

**Beneficios:**
- ðŸ”¹ Reduce footprint de deployments
- ðŸ”¹ Evita timeouts por falta de recursos
- ðŸ”¹ Cada funciÃ³n optimizada para su propÃ³sito

## Comportamiento en Vercel

### âœ… Funciona Correctamente
```
POST /api/scrape
â†’ scrapeWithFetch()
â†’ fetch("https://www.polenes.cl/...")
â†’ Extrae concentraciones del HTML
â†’ Guarda en Supabase
â†’ Response: { success: true, recordId: X }
```

### Limitaciones Conocidas
- polenes.cl usa JavaScript para generar contenido
- `fetch()` recibe HTML estÃ¡tico sin datos actualizados
- SoluciÃ³n: Ver "Alternativas" abajo

## Alternativas si fetch no funciona

### OpciÃ³n 1: Scraping Local + API Fetch
```bash
# En tu mÃ¡quina local (cron job)
0 */6 * * * npm --prefix /ruta/proyecto run scrape:save
```
- El endpoint `/api/scrape` solo retorna Ãºltimo dato guardado
- Supabase actÃºa como cachÃ© distribuida

### OpciÃ³n 2: API Scraping Service
Usar servicio externo como:
- **ScraperAPI**: API que maneja JavaScript rendering
- **Bright Data**: RotaciÃ³n de proxies + rendering
- **Apify**: Plataforma de scraping serverless

```typescript
async function scrapeWithAPI(): Promise<PollenData> {
  const response = await fetch(
    `https://api.scraperapi.com?api_key=${API_KEY}&url=https://www.polenes.cl/...`
  );
  return parseResponse(await response.json());
}
```

### OpciÃ³n 3: Vercel Cron (Cancelado - Vercel Pro)
Requiere plan Pro (costo $20/mes)

## RecomendaciÃ³n

**Para producciÃ³n actual:**
1. Usar `npm run scrape:save` manualmente desde local
2. Documentar en README instrucciones para usuarios
3. El endpoint `/api/scrape` sirve como "Force Refresh" para desarrollo

**Para futuro:**
- Si polenes.cl implementa una API pÃºblica â†’ usar esa
- Si crece el proyecto â†’ considerar ScraperAPI
- Mantener estrategia hÃ­brida para flexibilidad

## Testing

```bash
# Desarrollo (usa Puppeteer)
npm run dev
curl http://localhost:3000/api/scrape

# ProducciÃ³n simulada (usa fetch)
VERCEL=1 npm run build
npm run preview
```

## Logs de VerificaciÃ³n

```
âœ… npm run scrape:save
ðŸ“¡ Scraping y guardado...
ConexiÃ³n con Supabase exitosa
ðŸ“¡ Scraping con Puppeteer...
ðŸ“¤ Guardando en Supabase...
âœ… Guardado con ID: 8

âœ… npm run build
[@astrojs/vercel] Bundling function ../../../../dist/server/entry.mjs
[build] Complete!
```

---

**Estado**: âœ… Implementado y Testeado
**Ãšltima actualizaciÃ³n**: 2025-11-15
